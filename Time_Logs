import time
from datetime import datetime, timedelta
import pandas as pd
import requests

# Call to ELia API for ods088 dataset (get timestamp and publication date) + create and rename csv
def get_data_ods088(dataset_id='ods088'):
    url = f'https://opendata.elia.be/api/v2/catalog/datasets/{dataset_id}/records?order_by=datetime%20desc&limit=60&offset=0&lang=en&timezone=UTC'
    timestamps = []
    system_imbalance = []
    datetimes = []
    resolutioncode = []
    netregulationvolume = []
    strategicreservevolume = []
    grossupwardregulationvolume = []
    r2volumeup = []
    r2volumedown = []
    igccvolumeup = []
    igccvolumedown = []
    bidsvolumeup = []
    bidsvolumedown = []
    r3stdvolume = []
    r3flexvolume = []
    grossdownwardregulationvolume = []
    
    try:
        get_data = requests.get(url)
        records = get_data.json()['records']
        timestamps = [record['record']['timestamp'] for record in records]
        system_imbalance = [record['record']['fields']['systemimbalance'] for record in records]
        datetimes = [record['record']['fields']['datetime'] for record in records]
        resolutioncode = [record['record']['fields']['resolutioncode'] for record in records]
        netregulationvolume = [record['record']['fields']['netregulationvolume'] for record in records]
        strategicreservevolume = [record['record']['fields']['strategicreservevolume'] for record in records]
        grossupwardregulationvolume = [record['record']['fields']['grossupwardregulationvolume'] for record in records]
        r2volumeup = [record['record']['fields']['r2volumeup'] for record in records]
        r2volumedown = [record['record']['fields']['r2volumedown'] for record in records]
        igccvolumeup = [record['record']['fields']['igccvolumeup'] for record in records]
        igccvolumedown = [record['record']['fields']['igccvolumedown'] for record in records]
        bidsvolumeup = [record['record']['fields']['bidsvolumeup'] for record in records]
        bidsvolumedown = [record['record']['fields']['bidsvolumedown'] for record in records]
        r3stdvolume = [record['record']['fields']['r3stdvolume'] for record in records]
        r3flexvolume = [record['record']['fields']['r3flexvolume'] for record in records]
        grossdownwardregulationvolume = [record['record']['fields']['grossdownwardregulationvolume'] for record in records]
    except:
        print("An exception has occurred")
    
    return timestamps, datetimes, system_imbalance, resolutioncode, netregulationvolume, strategicreservevolume, grossupwardregulationvolume, r2volumeup, r2volumedown, igccvolumeup, igccvolumedown, bidsvolumeup, bidsvolumedown, r3stdvolume, r3flexvolume, grossdownwardregulationvolume

# Call to ELia API for ods077 dataset (get timestamp and publication date) + create and rename csv 

def get_data_ods077(dataset_id='ods077'):
    url = f'https://opendata.elia.be/api/v2/catalog/datasets/{dataset_id}/records?order_by=datetime%20desc&limit=60&offset=0&lang=en&timezone=UTC'
    timestamps = []
    datetimes = []
    resolutioncode = []
    quarterhour = []
    qualitystatus = []
    systemimbalance = []
    netregulationvolume = []
    alpha = []
    marginalincrementalprice = []
    marginaldecrementalprice = []
    strategicreserveprice = []
    systemimbalancevsincrementalbidscoordinable = []
    positiveimbalanceprice = []
    negativeimbalanceprice = []
    
    
    try:
        get_data = requests.get(url)
        records = get_data.json()['records']
        timestamps = [record['record']['timestamp'] for record in records]
        datetimes = [record['record']['fields']['datetime'] for record in records]
        resolutioncode = [record['record']['fields']['resolutioncode'] for record in records]
        quarterhour = [record['record']['fields']['quarterhour'] for record in records]
        qualitystatus = [record['record']['fields']['qualitystatus'] for record in records]
        systemimbalance = [record['record']['fields']['systemimbalance'] for record in records]
        netregulationvolume = [record['record']['fields']['netregulationvolume'] for record in records]
        alpha = [record['record']['fields']['alpha'] for record in records]
        marginalincrementalprice = [record['record']['fields']['marginalincrementalprice'] for record in records]
        marginaldecrementalprice = [record['record']['fields']['marginaldecrementalprice'] for record in records]
        strategicreserveprice = [record['record']['fields']['strategicreserveprice'] for record in records]
        systemimbalancevsincrementalbidscoordinable = [record['record']['fields']['systemimbalancevsincrementalbidscoordinable'] for record in records]
        positiveimbalanceprice = [record['record']['fields']['positiveimbalanceprice'] for record in records]
        negativeimbalanceprice = [record['record']['fields']['negativeimbalanceprice'] for record in records]


    except:
        print("An exception has occured")
        timestamps,datetimes,resolutioncode,quarterhour,qualitystatus,systemimbalance, netregulationvolume, alpha,marginalincrementalprice, marginaldecrementalprice,strategicreserveprice, systemimbalancevsincrementalbidscoordinable, positiveimbalanceprice, negativeimbalanceprice = [], [], [], [],[], [], [], [],[], [], [], [],[], []

    return timestamps,datetimes,resolutioncode,quarterhour,qualitystatus,systemimbalance, netregulationvolume, alpha,marginalincrementalprice, marginaldecrementalprice,strategicreserveprice, systemimbalancevsincrementalbidscoordinable, positiveimbalanceprice, negativeimbalanceprice

# Call to ELia API for ods088 dataset (get timestamp and publication date) + create and rename csv 

def get_data_ods078(dataset_id='ods078'):
    url = f'https://opendata.elia.be/api/v2/catalog/datasets/{dataset_id}/records?order_by=datetime%20desc&limit=60&offset=0&lang=en&timezone=UTC'
    timestamps = []
    datetimes = []
    qualitystatus = []
    resolutioncode = []
    netregulationvolume = []
    systemimbalance = []
    alpha = []
    marginalincrementalprice = []
    marginaldecrementalprice = []
    positiveimbalanceprice = []
    negativeimbalanceprice = []
    
    
    
    try:
        get_data = requests.get(url)
        records = get_data.json()['records']
        timestamps = [record['record']['timestamp'] for record in records]
        datetimes = [record['record']['fields']['datetime'] for record in records]
        qualitystatus = [record['record']['fields']['qualitystatus'] for record in records]
        resolutioncode = [record['record']['fields']['resolutioncode'] for record in records]
        netregulationvolume = [record['record']['fields']['netregulationvolume'] for record in records]
        systemimbalance = [record['record']['fields']['systemimbalance'] for record in records]
        alpha = [record['record']['fields']['alpha'] for record in records]
        marginalincrementalprice = [record['record']['fields']['marginalincrementalprice'] for record in records]
        marginaldecrementalprice = [record['record']['fields']['marginaldecrementalprice'] for record in records]
        positiveimbalanceprice = [record['record']['fields']['positiveimbalanceprice'] for record in records]
        negativeimbalanceprice = [record['record']['fields']['negativeimbalanceprice'] for record in records]
        
        
        
    except:
        print("An exception has occured")
        timestamps,datetimes,resolutioncode,qualitystatus,systemimbalance, netregulationvolume, alpha,marginalincrementalprice, marginaldecrementalprice, positiveimbalanceprice, negativeimbalanceprice = [], [], [], [],[], [], [], [],[], [], []

    return timestamps,datetimes,qualitystatus, resolutioncode,netregulationvolume,systemimbalance, alpha,marginalincrementalprice, marginaldecrementalprice, positiveimbalanceprice, negativeimbalanceprice


# Main Function: API calling to final datasets creation

#create an empty DataFrame with the desired columns
columns1 = ['datetime', 'timestamp', 'Delta', "Datetime_Now", 'resolutioncode', 'systemimbalance', 'netregulationvolume', 'strategicreservevolume', 'grossupwardregulationvolume', 'r2volumeup', 'r2volumedown', 'igccvolumeup', 'igccvolumedown', 'bidsvolumeup', 'bidsvolumedown', 'r3stdvolume', 'r3flexvolume', 'grossdownwardregulationvolume']
Records_Table = pd.DataFrame(columns=columns1)

# set the start and end time
start_time = datetime.now()
end_time = start_time + timedelta(minutes=1000000)

#create an empty DataFrame with the desired columns
columns2 = ['datetime', 'timestamp', 'Delta', "Datetime_Now","resolutioncode","quarterhour","qualitystatus","systemimbalance", "netregulationvolume", "alpha","marginalincrementalprice", "marginaldecrementalprice","strategicreserveprice", "systemimbalancevsincrementalbidscoordinable", "positiveimbalanceprice", "negativeimbalanceprice"]
Records_Table1 = pd.DataFrame(columns=columns2)

# set the start and end time
start_time1 = datetime.now()
end_time1 = start_time1 + timedelta(minutes= 1000000)

#create an empty DataFrame with the desired columns
columns3 =['datetime', 'timestamp', 'Delta', "Datetime_Now","qualitystatus","resolutioncode","netregulationvolume","systemimbalance", "alpha","marginalincrementalprice", "marginaldecrementalprice", "positiveimbalanceprice", "negativeimbalanceprice"]
Records_Table2 = pd.DataFrame(columns=columns3)

# set the start and end time
start_time2 = datetime.now()
end_time2 = start_time2 + timedelta(minutes= 1000000)

# Set the current month and year for filename
current_day = datetime.now().day
current_month = datetime.now().month
current_year = datetime.now().year
Date_Now = str(current_day) + "_" + str(current_month) + "_" + str(current_year)

# update the DataFrame every minute until the end time is reached
while datetime.now() < end_time:
    # Get timestamp and publication date data
    timestamps, datetimes, system_imbalance, resolutioncode, netregulationvolume, strategicreservevolume, grossupwardregulationvolume, r2volumeup, r2volumedown, igccvolumeup, igccvolumedown, bidsvolumeup, bidsvolumedown, r3stdvolume, r3flexvolume, grossdownwardregulationvolume = get_data_ods088()
    
    if len(timestamps) > 0:
        # Print execution time for each API call
        start_time = time.time()
        print("\n --- Execution time : %s seconds --- \n" % (time.time() - start_time))
        
        print("\n --- Dataset updating --- \n")
        
        # Create pandas dataset from retrieved data 
        result = pd.DataFrame({
            'timestamp': timestamps, 'datetime': datetimes, 'system_imbalance': system_imbalance,
            'resolution_code': resolutioncode, 'netregulationvolume': netregulationvolume,
            'strategicreservevolume': strategicreservevolume,
            'grossupwardregulationvolume': grossupwardregulationvolume, 'r2volumeup': r2volumeup,
            'r2volumedown': r2volumedown, 'igccvolumeup': igccvolumeup, 'igccvolumedown': igccvolumedown,
            'bidsvolumeup': bidsvolumeup, 'bidsvolumedown': bidsvolumedown, 'r3stdvolume': r3stdvolume,
            'r3flexvolume': r3flexvolume, 'grossdownwardregulationvolume': grossdownwardregulationvolume
        })

        # Create Delta column = 0
        result["Delta"] = 0
        
        # Create Datetime Now column
        result["Datetime_Now"] = datetime.now()

        # Change Timestamp and Datetime types to pydatetime (UTC + 1)
        result['timestamp'] = pd.to_datetime(result['timestamp'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
        result['datetime'] = pd.to_datetime(result['datetime'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
        result["Datetime_Now"] = pd.to_datetime(result['Datetime_Now'], errors='coerce').dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')

        # Set Delta to the difference in seconds between Timestamp and Datetime
        result["Delta"] = (result['timestamp'] - result['datetime']).dt.total_seconds() - 60
        
        # Append all rows to Records_Table
        Records_Table = Records_Table.append(result, ignore_index=True)
                                    
        # Drop duplicates
        merged_data_ODS088 = Records_Table.drop_duplicates(subset="datetime")
        merged_data_ODS088 = merged_data_ODS088.sort_values('datetime')

        # Write the merged data to a CSV file
        merged_data_ODS088.to_csv("/home/jovyan/Hackathon/ODS_files/all_dimensions_ods088.csv", index=False)
        
        
# while datetime.now() < end_time1:

# Get timestamp and publication date data
    # timestamp, publication_date , system_imbalance = get_data_ods088()
    timestamps,datetimes,resolutioncode,quarterhour,qualitystatus,systemimbalance, netregulationvolume, alpha,marginalincrementalprice, marginaldecrementalprice,strategicreserveprice, systemimbalancevsincrementalbidscoordinable, positiveimbalanceprice, negativeimbalanceprice = get_data_ods077()
    # timestamp2, publication_date2 , system_imbalance2 = get_data_ods078()

    if len(timestamps) > 0:
        
        # Print execution time for each API call
        start_time1 = time.time()
        print("\n --- Execution time : %s seconds --- \n" % (time.time() - start_time1))
        
        print("\n --- Dataset updating --- \n")
        
        # Create pandas dataset from retrieved data 
        result1 = pd.DataFrame({'datetime': datetimes, 'timestamp': timestamps, 'resolutioncode' : resolutioncode,
                                'quarterhour' : quarterhour, 'qualitystatus': qualitystatus, 'systemimbalance': systemimbalance, 'netregulationvolume' : netregulationvolume,
                                'alpha': alpha, 'marginalincrementalprice': marginalincrementalprice, 'marginaldecrementalprice':marginaldecrementalprice,
                                'strategicreserveprice':strategicreserveprice,'systemimbalancevsincrementalbidscoordinable' : systemimbalancevsincrementalbidscoordinable,
                                'positiveimbalanceprice' : positiveimbalanceprice,'negativeimbalanceprice':negativeimbalanceprice})

        ## Merge datasets, Create Delta & Apply data types
        # Create Delta column = 0
        result1["Delta"] = 0
        
        # Create Datetime Now columns
        result1["Datetime_Now"] = datetime.now()

        # Change Timestamp and Datetime types to pydatetime (UTC + 1)
        result1['timestamp'] = pd.to_datetime(result1['timestamp'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
        result1['datetime'] = pd.to_datetime(result1['datetime'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
        result1["Datetime_Now"] = pd.to_datetime(result1['Datetime_Now'], errors='coerce').dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')

        # Set Delta to the difference in seconds between Timestamp and Datetime
        result1["Delta"] = ((result1['timestamp'] - result1['datetime']).dt.total_seconds())-60
        
        # Select the first row and only keep necessary columns
        # first_row = result.iloc[0][["timestamp", "datetime", "systemimbalance", "Delta", "Datetime_Now"]]
        
#         # Select all 60 values
#         rows1 = result1.iloc[:][['datetime', 'timestamp', 'Imbalance_Price', 'resolutioncode', 'quarterhour', 'qualitystatus', 'systemimbalance',
#                                  'netregulationvolume', 'alpha', 'marginalincrementalprice', 'marginaldecrementalprice','strategicreserveprice',
#                                  'systemimbalancevsincrementalbidscoordinable', 'negativeimbalanceprice']]

        # Append the first row to Records_Table
        # Records_Table = Records_Table.append(first_row, ignore_index=True)
        # print(Records_Table)
        
        # Append all rows to Records_Table
        Records_Table1 = Records_Table1.append(result1, ignore_index=True)
        
        # Drop duplicates
        merged_data_ODS077 = Records_Table1.drop_duplicates(subset="datetime")
        merged_data_ODS077 = merged_data_ODS077.sort_values('datetime')

#         hist_df1 = pd.DataFrame(pd.read_csv("\\\\isofile05\\SMDBA\\07-Public\\Hackathon\\Selected_rows_ODS077_20_3_2023V2.csv"))
#         merged_data_ODS077 = hist_df1.append(merged_data_ODS077)
        # Write the merged data to a CSV file
        merged_data_ODS077.to_csv("/home/jovyan/Hackathon/ODS_files/all_dimensions_ods077.csv", index=False)
        
        # Records_Table1.to_csv(r'/home/jovyan/Hackathon/CSV_files/All_rows_ODS077_' + str(Date_Now) + 'V2.csv', index=False)
        # NR1.to_csv(r'/home/jovyan/Hackathon/CSV_files/Selected_rows_ODS077_' + Date_Now + 'V2.csv', index=False)

# while datetime.now() < end_time2:

    # Get timestamp and publication date data
    # timestamp, publication_date , system_imbalance = get_data_ods088()
    # timestamp1, publication_date1 , system_imbalance1 = get_data_ods077()
    timestamps,datetimes,qualitystatus, resolutioncode,netregulationvolume,systemimbalance, alpha,marginalincrementalprice, marginaldecrementalprice, positiveimbalanceprice, negativeimbalanceprice = get_data_ods078()
    
    if len(timestamps) > 0:
        
        # Print execution time for each API call
        start_time2 = time.time()
        print("\n --- Execution time : %s seconds --- \n" % (time.time() - start_time2))
        
        print("\n --- Dataset updating --- \n")
        
        # Create pandas dataset from retrieved data 
        result2 = pd.DataFrame({'datetime': datetimes, 'timestamp': timestamps, 'qualitystatus': qualitystatus, 'resolutioncode' : resolutioncode,
                                'netregulationvolume' : netregulationvolume,'systemimbalance': systemimbalance,
                                'alpha': alpha, 'marginalincrementalprice': marginalincrementalprice, 'marginaldecrementalprice':marginaldecrementalprice,
                                'positiveimbalanceprice' : positiveimbalanceprice,'negativeimbalanceprice':negativeimbalanceprice})

        # Merge datasets, Create Delta & Apply data types
        # Create Delta column = 0
        result2["Delta"] = 0
        
        # Create Datetime Now columns
        result2["Datetime_Now"] = datetime.now()

        # Change Timestamp and Datetime types to pydatetime (UTC + 1)
        try:
            result2['timestamp'] = pd.to_datetime(result2['timestamp'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
            result2['datetime'] = pd.to_datetime(result2['datetime'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
            result2["Datetime_Now"] = pd.to_datetime(result2['Datetime_Now'], errors='coerce').dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')
        except Exception as e :
            print(e)
            continue

#         result2['timestamp'] = pd.to_datetime(result2['timestamp'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
#         result2['datetime'] = pd.to_datetime(result2['datetime'], errors='coerce').dt.tz_convert('UTC').dt.tz_convert('Europe/Paris')
#         result2["Datetime_Now"] = pd.to_datetime(result2['Datetime_Now'], errors='coerce').dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')
    
        # Set Delta to the difference in seconds between Timestamp and Datetime
        result2["Delta"] = ((result2['timestamp'] - result2['datetime']).dt.total_seconds())-900
        
        # Select the first row and only keep necessary columns
        # first_row = result.iloc[0][["timestamp", "datetime", "systemimbalance", "Delta", "Datetime_Now"]]
        
#         # Select all 60 values
#         rows2 = result2.iloc[:][["datetime", "timestamp", "Delta", "Imbalance_Price", "Datetime_Now"]]

        # Append the first row to Records_Table
        # Records_Table = Records_Table.append(first_row, ignore_index=True)
        # print(Records_Table)
        
        # Append all rows to Records_Table
        Records_Table2 = Records_Table2.append(result2, ignore_index=True)
        
        # Drop duplicates
        merged_data_ODS078 = Records_Table2.drop_duplicates(subset="datetime")
        merged_data_ODS078 = merged_data_ODS078.sort_values('datetime')

#         hist_df2 = pd.DataFrame(pd.read_csv("\\\\isofile05\\SMDBA\\07-Public\\Hackathon\\Selected_rows_ODS078_20_3_2023V2.csv"))
#         merged_data_ODS078 = hist_df2.append(merged_data_ODS078)
        # Write the merged data to a CSV file
        merged_data_ODS078.to_csv("/home/jovyan/Hackathon/ODS_files/all_dimensions_ods078.csv", index=False)      
        
    # Repeat the function every 60 seconds
    time.sleep(60)
